(window.webpackJsonp=window.webpackJsonp||[]).push([[61],{388:function(a,e,t){"use strict";t.r(e);var r=t(3),n=Object(r.a)({},(function(){var a=this,e=a._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h2",{attrs:{id:"论文"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#论文"}},[a._v("#")]),a._v(" 论文")]),a._v(" "),e("h3",{attrs:{id:"摘要"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#摘要"}},[a._v("#")]),a._v(" 摘要")]),a._v(" "),e("p",[a._v("一般重建结果比较好的方法都需要几个小时的离线处理。最近在线重建的效果还没那么好，有以下问题")]),a._v(" "),e("ul",[e("li",[a._v("需要几分钟的处理，不能达到实时；")]),a._v(" "),e("li",[a._v("脆弱的跟踪模型，导致跟踪失败；")]),a._v(" "),e("li",[a._v("仅支持基于点的非结构化表示，这限制了扫描质量和适用性。")])]),a._v(" "),e("p",[a._v("我们消除了对时间跟踪的严重依赖，而是不断定位到全局优化的帧")]),a._v(" "),e("h3",{attrs:{id:"介绍"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#介绍"}},[a._v("#")]),a._v(" 介绍")]),a._v(" "),e("p",[a._v("大规模实时3D重建问题需要满足以下条件")]),a._v(" "),e("p",[a._v("高质量的表面模型")]),a._v(" "),e("p",[a._v("​\t\t我们需要一个单一的纹理和无噪点 3D 场景模型，可由标准图形应用程序使用。 这需要一个可以模拟连续表面而不是离散点的高质量表示。")]),a._v(" "),e("p",[a._v("可扩展性")]),a._v(" "),e("p",[a._v("全局模型一致性")]),a._v(" "),e("p",[a._v("​\t\t随着规模的发函，需要纠正姿态漂移和估计误差。")]),a._v(" "),e("p",[a._v("鲁棒的相机跟踪")]),a._v(" "),e("p",[a._v("​\t\t在特征不明显的区域可能跟踪失败，需要有重定位能力。许多先有的方法是基于与前一阵的接近度，从而限制了相机快速运动。我们就需要在不依赖时间连贯性的情况下的稳健的方法重定位。")]),a._v(" "),e("p",[a._v("即时模型更新")]),a._v(" "),e("p",[a._v("​\t\t根据最新的姿态估计更新模型。")]),a._v(" "),e("p",[e("strong",[a._v("由于我们全局关联每个 RGB-D 帧，因此可以隐式且连续地处理回环，从而无需任何显式回环检测。")])]),a._v(" "),e("h3",{attrs:{id:"方法概述"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#方法概述"}},[a._v("#")]),a._v(" 方法概述")]),a._v(" "),e("p",[a._v("我们使用一组稀疏特征对应来获得粗略的全局对齐，因为稀疏特征固有地提供了回环检测和重新定位。然后通过优化密集的光度和几何一致性来细化这种对齐方式。")]),a._v(" "),e("p",[a._v("提取SIFT关键点，和以前所有帧进行匹配，并且严格的取出无匹配。")]),a._v(" "),e("p",[a._v("使用过滤后的关联，分层地局部到全局的位姿优化方法，在第一层，连续n帧（时间）组成一个chunk，在块内进行局部位姿优化。在第二层，所有的chunk相互关联进行全局优化。这种优化策略减少了未知量保证大范围重建。")]),a._v(" "),e("p",[a._v("为了使用改进的估计来跟新帧的位姿，使用新的实时分离步骤移除旧位姿的RGB-D图像，并在新位姿处重新整合。")]),a._v(" "),e("h3",{attrs:{id:"寻找特征关联"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#寻找特征关联"}},[a._v("#")]),a._v(" 寻找特征关联")]),a._v(" "),e("p",[a._v("对于新来的每一帧，提取SIFT特征，和以前所有帧进行匹配。之后筛选没两帧之间的匹配，取出匹配，并生成有效的成对关联的列表，作为全局位姿优化的输入。")]),a._v(" "),e("h4",{attrs:{id:"去除两帧之间的关联"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#去除两帧之间的关联"}},[a._v("#")]),a._v(" 去除两帧之间的关联")]),a._v(" "),e("p",[a._v("为了最小化外点，根据几何和光度一致性过滤检测到的成对对应集。在进行优化后算出一个残差，如果大于某个值，就移除这两帧之间所有的匹配，另外，如果某一帧与任何一帧都没有关联，则称为无效帧。")]),a._v(" "),e("h4",{attrs:{id:"去除关键点误匹配"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#去除关键点误匹配"}},[a._v("#")]),a._v(" 去除关键点误匹配")]),a._v(" "),e("p",[a._v("当前关联Pcur，Qcur，对于新添加的关联，p,q，计算出一个变换，使得Pcur, Qcur的RMSD最小。之后计算Pcur，Qcur的一些有关误差的数，如果大于一个阈值，贼认为系统是不稳定的，那么就移除这些关联，直到这不够计算出一个变换。如果计算不出一个变换，那么就把这两帧之间的所有关联移除。")]),a._v(" "),e("h4",{attrs:{id:"通过表面过滤"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#通过表面过滤"}},[a._v("#")]),a._v(" 通过表面过滤")]),a._v(" "),e("p",[a._v("检查特征跨越的表面是否足够大，因为跨越太小的尺寸的关联会引起歧义。对于每组3D点，我们将它们投影到由它们的两个主轴方向（可以用中心距和重心求出）给定的平面中，表面积由生成的投影点的2D定向边界框给定。如果跨越的区域不足多少平米，则该组匹配被丢弃。")]),a._v(" "),e("h4",{attrs:{id:"密集验证"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#密集验证"}},[a._v("#")]),a._v(" 密集验证")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/lzxiaohuihui/piture_for_markdown/main/img/image-20220214095619822.png",alt:"image-20220214095619822"}})]),a._v(" "),e("p",[a._v("以上式子都通过，就添加这关联到有效的关联集中，这之后用来做位姿优化。如果两帧之间关联数量大于一定值，则只使用帧间匹配来估计位姿。")]),a._v(" "),e("h3",{attrs:{id:"分层优化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#分层优化"}},[a._v("#")]),a._v(" 分层优化")]),a._v(" "),e("p",[a._v("在输入的图像序列中，连续n帧组成一个chunk。在底层优化中，在一个块内进行局部对齐。在第二层，每一块的第一帧，作为这个块的关键帧，全局优化就是，对齐这些关键帧。")]),a._v(" "),e("h4",{attrs:{id:"块内局部对齐"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#块内局部对齐"}},[a._v("#")]),a._v(" 块内局部对齐")]),a._v(" "),e("p",[a._v("基于输入RGB-D流中连续11个帧，组成一个块，相邻的块重叠1帧。局部位姿优化是，计算与参考帧（块的第一帧）最优的相对位姿。优化的方法是，首先是上面的寻找特征关联，并且使用能量函数")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/lzxiaohuihui/piture_for_markdown/main/img/image-20220214104547678.png",alt:"image-20220214104547678"}})]),a._v(" "),e("p",[e("mjx-container",{staticClass:"MathJax",staticStyle:{direction:"ltr",position:"relative"},attrs:{jax:"SVG"}},[e("svg",{staticStyle:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.357ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"5.841ex",height:"1.359ex",role:"img",focusable:"false",viewBox:"0 -443 2581.6 600.8","aria-hidden":"true"}},[e("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"}},[e("g",{attrs:{"data-mml-node":"math"}},[e("g",{attrs:{"data-mml-node":"msub"}},[e("g",{attrs:{"data-mml-node":"mi"}},[e("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D464",d:"M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"}})]),e("g",{attrs:{"data-mml-node":"TeXAtom",transform:"translate(749,-150) scale(0.707)","data-mjx-texclass":"ORD"}},[e("g",{attrs:{"data-mml-node":"mi"}},[e("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D451",d:"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"}})]),e("g",{attrs:{"data-mml-node":"mi",transform:"translate(520,0)"}},[e("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D452",d:"M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"}})]),e("g",{attrs:{"data-mml-node":"mi",transform:"translate(986,0)"}},[e("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D45B",d:"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"}})]),e("g",{attrs:{"data-mml-node":"mi",transform:"translate(1586,0)"}},[e("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D460",d:"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"}})]),e("g",{attrs:{"data-mml-node":"mi",transform:"translate(2055,0)"}},[e("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D452",d:"M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"}})])])])])])]),e("mjx-assistive-mml",{staticStyle:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"},attrs:{unselectable:"on",display:"inline"}},[e("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[e("msub",[e("mi",[a._v("w")]),e("mrow",{attrs:{"data-mjx-texclass":"ORD"}},[e("mi",[a._v("d")]),e("mi",[a._v("e")]),e("mi",[a._v("n")]),e("mi",[a._v("s")]),e("mi",[a._v("e")])],1)],1)],1)],1)],1),a._v("是线性增加的，这让离散项找到一个比较好的全局结构，然后使用稠密项优化。")],1),a._v(" "),e("p",[a._v("在离散的匹配项，最小化S中所有帧对，所有关联的特征，投影到世界坐标系，的差。")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/lzxiaohuihui/piture_for_markdown/main/img/image-20220214105725873.png",alt:"image-20220214105725873"}})]),a._v(" "),e("p",[a._v("C(i, j)是第i帧和第j帧关联的特征。")]),a._v(" "),e("p",[a._v("之后对块进行密集验证，如果重投影误差大于阈值，则丢弃这个块。")]),a._v(" "),e("h4",{attrs:{id:"关键帧优化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#关键帧优化"}},[a._v("#")]),a._v(" 关键帧优化")]),a._v(" "),e("p",[a._v("一旦一个块完成优化，那么就定义块内第一帧作为关键帧。获得一个"),e("strong",[a._v("关键帧特征集")]),a._v("，一旦获得关键帧特征集，就丢弃块数据。")]),a._v(" "),e("h4",{attrs:{id:"全局块间优化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#全局块间优化"}},[a._v("#")]),a._v(" 全局块间优化")]),a._v(" "),e("p",[a._v("一旦一个关键帧没有找到一个以前的帧作为匹配，那么就视为无效帧，之后可以恢复。和局部块内优化一样，使用离散项和密集项来优化全局位姿。")]),a._v(" "),e("h2",{attrs:{id:"参考文献"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考文献"}},[a._v("#")]),a._v(" 参考文献")]),a._v(" "),e("h4",{attrs:{id:"基于非结构化点的表示"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#基于非结构化点的表示"}},[a._v("#")]),a._v(" 基于非结构化点的表示")]),a._v(" "),e("p",[a._v("P. Henry, M. Krainin, E. Herbst, X. Ren, and D. Fox. 2010. RGB-D mapping: Using depth cameras for dense 3D modeling of indoor environments. In Proc. Int. Symp. Experimental Robotics, Vol. 20. 22–25.")]),a._v(" "),e("p",[a._v("Maik Keller, Damien Leoch, Martin Lambers, Shahram Izadi, Tim Weyrich, and Andreas Kolb. 2013. Real-time 3D Reconstruction in Dynamic Scenes using Point-based Fusion. In Proc. 3DV. IEEE, 1–8.")]),a._v(" "),e("p",[a._v("T. Weise, T. Wismer, B. Leibe, and L. Van Gool. 2009. In-hand scanning with online loop closure. In Proc. ICCV Workshops. 1630–1637.")]),a._v(" "),e("p",[a._v("T. Whelan, S. Leutenegger, R. F. Salas-Moreno, B. Glocker, and A. J. Davison. 2015. ElasticFusion: Dense SLAM Without A Pose Graph. In Proc. RSS. Rome, Italy.")]),a._v(" "),e("h4",{attrs:{id:"基于2-5d深度图"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#基于2-5d深度图"}},[a._v("#")]),a._v(" 基于2.5D深度图")]),a._v(" "),e("p",[a._v("Maxime Meilland, Andrew Comport, and others. 2013. On unifying key-frame and voxel-based dense visual SLAM at large scales. In Proc. IROS. IEEE, 3677–3683.")]),a._v(" "),e("p",[a._v("P. Merrell, A. Akbarzadeh, L. Wang, P. Mordohai, J.M. Frahm, R. Yang, D. Nister, ´ and M. Pollefeys. 2007. Real-time visibility-based fusion of depth maps. In Proc. ICCV. 1–8.")]),a._v(" "),e("h4",{attrs:{id:"基于高度场"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#基于高度场"}},[a._v("#")]),a._v(" 基于高度场")]),a._v(" "),e("p",[a._v("David Gallup, Marc Pollefeys, and Jan-Michael Frahm. 2010. 3D reconstruction using an n-layer heightmap. In Paern Recognition. Springer, 1–10.")]),a._v(" "),e("h4",{attrs:{id:"基于占用网格的体积"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#基于占用网格的体积"}},[a._v("#")]),a._v(" 基于占用网格的体积")]),a._v(" "),e("p",[a._v("Alberto Elfes and Larry Mahies. 1987. Sensor integration for robot navigation: combining sonar and stereo range data in a grid-based representataion. In Decision and Control, 1987. 26th IEEE Conference on, Vol. 26. IEEE, 1802–1807.")]),a._v(" "),e("p",[a._v("Kai M Wurm, Armin Hornung, Maren Bennewitz, Cyrill Stachniss, and Wolfram Burgard. 2010. OctoMap: A probabilistic, exible, and compact 3D map representation for robotic systems. In Proc. ICRA, Vol. 2.")]),a._v(" "),e("h4",{attrs:{id:"基于隐式曲面"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#基于隐式曲面"}},[a._v("#")]),a._v(" 基于隐式曲面")]),a._v(" "),e("p",[a._v("Brian Curless and Marc Levoy. 1996. A volumetric method for building complex models from range images. In In Proc. SIGGRAPH. ACM, 303–312.")]),a._v(" "),e("p",[a._v("A. Hilton, A. Stoddart, J. Illingworth, and T. Windea. 1996. Reliable surface reconstruction from multiple range images. JProc. ECCV (1996), 117–126.")]),a._v(" "),e("h4",{attrs:{id:"基于tsdf-隐式截断符号距离"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#基于tsdf-隐式截断符号距离"}},[a._v("#")]),a._v(" 基于TSDF（隐式截断符号距离）")]),a._v(" "),e("p",[a._v("Nicola Fioraio, Jonathan Taylor, Andrew Fitzgibbon, Luigi Di Stefano, and Shahram Izadi. 2015. Large-Scale and Dri-Free Surface Reconstruction Using Online Subvolume Registration. Proc. CVPR (June 2015).")]),a._v(" "),e("p",[a._v("Simon Fuhrmann and Michael Goesele. 2014. Floating Scale Surface Reconstruction. In Proc. SIGGRAPH.")]),a._v(" "),e("p",[a._v("Marc Levoy, Kari Pulli, Brian Curless, Szymon Rusinkiewicz, David Koller, Lucas Pereira, Ma Ginzton, Sean Anderson, James Davis, Jeremy Ginsberg, and others. 2000. e digital Michelangelo project: 3D scanning of large statues. In In Proc. SIGGRAPH. ACM Press/Addison-Wesley Publishing Co., 131–144.")]),a._v(" "),e("h4",{attrs:{id:"最近最突出的例子是-kinectfusion-20-34-其中展示了较小场景的实时体积融合。"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#最近最突出的例子是-kinectfusion-20-34-其中展示了较小场景的实时体积融合。"}},[a._v("#")]),a._v(" 最近最突出的例子是 KinectFusion [20, 34]，其中展示了较小场景的实时体积融合。")]),a._v(" "),e("p",[a._v("S. Izadi, D. Kim, O. Hilliges, D. Molyneaux, R. Newcombe, P. Kohli, J. Shoon, S. Hodges, D. Freeman, A. Davison, and A. Fitzgibbon. 2011. KinectFusion: Realtime 3D reconstruction and interaction using a moving depth camera. In Proc. UIST. 559–568.")]),a._v(" "),e("p",[a._v("Richard A Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, Andrew J Davison, Pushmeet Kohli, Jamie Shoon, Steve Hodges, and Andrew Fitzgibbon. 2011. KinectFusion: Real-time dense surface mapping and tracking. In Proc. ISMAR. 127–136.")]),a._v(" "),e("h4",{attrs:{id:"用于体积融合的实时高效数据结构。"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#用于体积融合的实时高效数据结构。"}},[a._v("#")]),a._v(" 用于体积融合的实时高效数据结构。")]),a._v(" "),e("p",[a._v("Jiawen Chen, Dennis Bautembach, and Shahram Izadi. 2013. Scalable real-time volumetric surface reconstruction. ACM TOG 32, 4 (2013), 113.")]),a._v(" "),e("p",[a._v("Maik Keller, Damien Leoch, Martin Lambers, Shahram Izadi, Tim Weyrich, and Andreas Kolb. 2013. Real-time 3D Reconstruction in Dynamic Scenes using Point-based Fusion. In Proc. 3DV. IEEE, 1–8.")]),a._v(" "),e("p",[a._v("M. Nießner, M. Zollhofer, S. Izadi, and M. Stamminger. 2013. Real-time 3D ¨ Reconstruction at Scale using Voxel Hashing. ACM TOG (2013).")]),a._v(" "),e("p",[a._v("F Reichl, J Weiss, and R Westermann. 2015. Memory-Ecient Interactive Online Reconstruction From Depth Image Streams. In Computer Graphics Forum. Wiley Online Library")]),a._v(" "),e("p",[a._v("H. Roth and M. Vona. 2012. Moving Volume KinectFusion. In Proc. BMVC")]),a._v(" "),e("p",[a._v("F. Steinbruecker, J. Sturm, and D. Cremers. 2014. Volumetric 3D Mapping in RealTime on a CPU. In 2014 IEEE International Conference on Robotics and Automation (ICRA). Hongkong, China.")]),a._v(" "),e("p",[a._v("T Whelan, H Johannsson, M Kaess, J Leonard, and J McDonald. 2012. Robust Tracking for Real-Time Dense RGB-D Mapping with Kintinuous. Technical Report. ery date: 2012-10-25.")]),a._v(" "),e("p",[a._v("M. Zeng, F. Zhao, J. Zheng, and X. Liu. 2012. Octree-based Fusion for Realtime 3D Reconstruction. Graphical Models (2012).")]),a._v(" "),e("p",[a._v("Yizhong Zhang, Weiwei Xu, Yiying Tong, and Kun Zhou. 2015. Online structure analysis for real-time indoor scene reconstruction. ACM Transactions on Graphics (TOG) 34, 5 (2015), 159.")]),a._v(" "),e("h4",{attrs:{id:"通过优化整个位姿轨迹来得到全局一致的模型"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#通过优化整个位姿轨迹来得到全局一致的模型"}},[a._v("#")]),a._v(" 通过优化整个位姿轨迹来得到全局一致的模型")]),a._v(" "),e("p",[a._v("Sungjoon Choi, Qian-Yi Zhou, and Vladlen Koltun. 2015. Robust Reconstruction of Indoor Scenes. Proc. CVPR (June 2015).")]),a._v(" "),e("p",[a._v("Hao Li, Etienne Vouga, Anton Gudym, Linjie Luo, Jonathan T Barron, and Gleb Gusev. 2013. 3D self-portraits. ACM TOG 32, 6 (2013), 187.")]),a._v(" "),e("p",[a._v("Qian-Yi Zhou and Vladlen Koltun. 2013. Dense scene reconstruction with points of interest. ACM Transactions on Graphics (TOG) 32, 4 (2013), 112.")]),a._v(" "),e("p",[a._v("Qian-Yi Zhou and Vladlen Koltun. 2014. Color map optimization for 3D reconstruction with consumer depth cameras. ACM Transactions on Graphics (TOG) 33, 4 (2014), 155.")]),a._v(" "),e("p",[a._v("Qian-Yi Zhou, Steven Miller, and Vladlen Koltun. 2013. Elastic fragments for dense scene reconstruction. In Computer Vision (ICCV), 2013 IEEE International Conference on. IEEE, 473–480")]),a._v(" "),e("h4",{attrs:{id:"实时单目rgb方法"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#实时单目rgb方法"}},[a._v("#")]),a._v(" 实时单目RGB方法")]),a._v(" "),e("h4",{attrs:{id:"稀疏"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#稀疏"}},[a._v("#")]),a._v(" 稀疏")]),a._v(" "),e("p",[a._v("Georg Klein and David Murray. 2007. Parallel Tracking and Mapping for Small AR Workspaces. In Proc. ISMAR. Nara, Japan")]),a._v(" "),e("h4",{attrs:{id:"半稠密"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#半稠密"}},[a._v("#")]),a._v(" 半稠密")]),a._v(" "),e("p",[a._v("Jakob Engel, Jurgen Sturm, and Daniel Cremers. 2013. Semi-dense visual odometry for a monocular camera. In Proc. ICCV. IEEE, 1449–1456.")]),a._v(" "),e("p",[a._v("Christian Forster, Matia Pizzoli, and Davide Scaramuzza. 2014. SVO: Fast semidirect monocular visual odometry. In Proc. ICRA. IEEE, 15–22.")]),a._v(" "),e("h4",{attrs:{id:"直接法"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#直接法"}},[a._v("#")]),a._v(" 直接法")]),a._v(" "),e("p",[a._v("J. Engel, T. Schops, and D. Cremers. 2014. LSD-SLAM: Large-Scale Direct Monoc- ¨ ular SLAM. In European Conference on Computer Vision.")]),a._v(" "),e("p",[a._v("Maxime Meilland, A Comport, Patrick Rives, and INRIA Sophia Antipolis Mediterran ´ ee. 2011. Real-time dense visual tracking under large lighting varia- ´ tions. In Proc. BMVC, Vol. 29.")]),a._v(" "),e("h4",{attrs:{id:"位姿图优化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#位姿图优化"}},[a._v("#")]),a._v(" 位姿图优化")]),a._v(" "),e("p",[a._v("Rainer Kummerle, Giorgio Grisei, Hauke Strasdat, Kurt Konolige, and Wolfram ¨ Burgard. 2011. g 2 o: A general framework for graph optimization. In Proc. ICRA. IEEE, 3607–3613.")]),a._v(" "),e("h4",{attrs:{id:"捆绑调整"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#捆绑调整"}},[a._v("#")]),a._v(" 捆绑调整")]),a._v(" "),e("p",[a._v("Bill Triggs, Philip F McLauchlan, Richard I Hartley, and Andrew W Fitzgibbon. 2000. Bundle adjustmen, a modern synthesis. In Vision algorithms: theory and practice. Springer, 298–372")]),a._v(" "),e("p",[a._v("MonoFusion [38] 通过密集体积融合增强了稀疏 SLAM 束调整，在小规模场景中显示出令人信服的单目结果。 实时 SLAM 方法通常首先逐帧估计位姿，然后在后台线程中执行校正（运行速度低于实时速率；例如，1Hz）。")]),a._v(" "),e("p",[a._v("Vivek Pradeep, Christoph Rhemann, Shahram Izadi, Christopher Zach, Michael Bleyer, and Steven Bathiche. 2013. MonoFusion: Real-time 3D reconstruction of small scenes with a single web camera. In Proc. ISMAR. 83–88.")]),a._v(" "),e("p",[a._v("相比之下，DTAM [35] 使用帧到模型跟踪的概念（来自 KinectFusion [20, 34]）直接从重建的密集 3D 模型估计姿势。 这省略了校正步骤的需要，但显然不能扩展到更大的场景。")]),a._v(" "),e("p",[a._v("Richard A. Newcombe, Steven J. Lovegrove, and Andrew J. Davison. 2011. DTAM: Dense Tracking and Mapping in Real-time. In Proc. ICCV. 2320–2327.")]),a._v(" "),e("h4",{attrs:{id:"icp"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#icp"}},[a._v("#")]),a._v(" ICP")]),a._v(" "),e("p",[a._v("P.J. Besl and N.D. McKay. 1992. A method for registration of 3-D shapes. IEEE Trans. PAMI 14, 2 (1992), 239–256.")]),a._v(" "),e("p",[a._v("S. Rusinkiewicz and M. Levoy. 2001. Ecient variants of the ICP algorithm. In Proc. 3DIM. 145–152.")])])}),[],!1,null,null,null);e.default=n.exports}}]);